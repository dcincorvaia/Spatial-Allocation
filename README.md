# Bumblebee Spatial Allocation Model

To test the hypothesis that using social information to allocate foraging effort in bumble bees is adaptive, we built an agent-based model in Python. The model consists of 100 agents (representing bees) foraging on a 100x100 landscape, in which are distributed 10 resource patches of different qualities. There are 2 patches each of quality 1,2,3,4, and 5, with 5 being the highest quality. This results in 0.1% of the landscape being covered by resources. 

	Each agent’s state represents the current value of the resource they are exploiting; at the beginning of a foraging trial, all agents are naive, hence have state 0.  Each trial consists of 100 updates; for each update, an agent may either search for a resource or continue to exploit a resource it has already found. At the beginning of a trial none of the agents have found a resource, so all must search until they have found a resource. Searching is implemented by placing the agent randomly in a space on the landscape and having them randomly walk for 100 steps, with each step being preceded by a  random turns. If an agent finds a resource during this search, its state changes to the value of that resource, which it continues to exploit on subsequent trials until another search is triggered. So, for example, if an agent with state 0 finds a resource of value 2, the agent’s state changes to 2, and on each subsequent update the agent will add 2 to the collected resource total. 
	
	Once an agent has found a resource, it may either continue to exploit it or search for a new resource. On each update there is a 1% chance that an agent will search instead of exploiting a resource. This is analogous to the underlying propensity for a bumble bee to randomly seek out novel resources instead of exploiting a known one (Heinrich 1979b). If search is triggered, the agent is randomly placed in the grid and then takes a  100-step  random walk. If this walk leads the  agent to encounter a resource that is higher in quality than its current state while searching, its state updates to the new value. If, for example, an agent with a state of 2 discovers a resource of value 3 while searching, that agent’s state changes from 2 to 3. If the agent finds nothing better while searching, it does not collect any resources that update, and continues to search on subsequent updates until it does find a better resource.  The effect of this is to impose a considerable opportunity cost for searching.
	
	Search can also be triggered through the use of social information: with an experimentally controlled probability, an agent will search if another agent from the colony has discovered a higher-valued state. If search is triggered this way it operates the same as the search described previously, and again there is an opportunity cost for searching instead of continuing to exploit a known patch.
Whether search is triggered spontaneously or via social encounters, each agent should have a chance to progress toward higher-valued rewards.  Our goal was to see whether social triggering of search would provide an advantage over purely individual initiative. In different conditions of the experiment, we varied the probability of social information sharing from 0 to 1, in increments of 0.2. For each of these information sharing probabilities, 1000 replicates were run with 200 updates each. To compare performance, we measured the total resources collected by the colony, as well as the distribution of resources discovered by each agent by the end of the trial.  
